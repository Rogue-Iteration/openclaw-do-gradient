#!/usr/bin/env python3
"""
Research data orchestrator for the Gradient Research Team.

Per-agent gather pipeline:
1. Runs the agent's gather scripts for each source
2. Uploads each result to DigitalOcean Spaces (separate files per source)
3. Triggers Knowledge Base re-indexing
4. Returns a summary for inter-agent notifications

Usage:
    # Nova's heartbeat
    python3 gather.py --ticker CAKE --name "The Cheesecake Factory" --agent nova --sources web,fundamentals

    # Ace's heartbeat
    python3 gather.py --ticker CAKE --name "The Cheesecake Factory" --agent ace --sources technicals

    # Dry run (gather only, no store)
    python3 gather.py --ticker CAKE --name "The Cheesecake Factory" --agent nova --sources web --dry-run

    # JSON output
    python3 gather.py --ticker CAKE --name "The Cheesecake Factory" --agent nova --sources web --json
"""

import argparse
import json
import os
import sys
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional

# â”€â”€â”€ Path Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_SCRIPTS_DIR = Path(__file__).parent
_SKILLS_ROOT = _SCRIPTS_DIR.parent.parent

# Add gather scripts directory to path
_GATHER_DIR = _SKILLS_ROOT / "gradient-data-gathering" / "scripts"
sys.path.insert(0, str(_GATHER_DIR))
sys.path.insert(0, str(_SCRIPTS_DIR))

# â”€â”€â”€ Source Registry â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Maps source name â†’ (module_name, gather_function_name)
SOURCE_REGISTRY = {
    "web": ("gather_web", "gather_web"),
    "fundamentals": ("gather_fundamentals", "gather_fundamentals"),
    "social": ("gather_social", "gather_social"),
    "technicals": ("gather_technicals", "gather_technicals"),
}

# Default sources per agent
AGENT_SOURCES = {
    "nova": ["web", "fundamentals"],
    "luna": ["social"],
    "ace": ["technicals"],
    "max": [],  # Max analyzes, doesn't gather
}

VALID_AGENTS = {"nova", "luna", "ace", "max"}


# â”€â”€â”€ Source Execution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def run_source(
    source: str,
    ticker: str,
    company_name: str,
    theme: Optional[str] = None,
    directive: Optional[str] = None,
) -> dict:
    """Run a single gather source and return its result.

    Returns:
        dict with keys:
        - source: source name
        - success: bool
        - markdown: the gathered markdown content (or error message)
        - metric_count: number of data points gathered (if available)
        - error: error message (if failed)
    """
    if source not in SOURCE_REGISTRY:
        return {
            "source": source,
            "success": False,
            "markdown": "",
            "metric_count": 0,
            "error": f"Unknown source: {source}",
        }

    module_name, func_name = SOURCE_REGISTRY[source]

    try:
        module = __import__(module_name)
        gather_func = getattr(module, func_name)

        # All gather functions accept (ticker, company_name, theme, directive)
        result = gather_func(ticker, company_name, theme=theme, directive=directive)

        markdown = result.get("markdown", "")
        metric_count = 0

        # Extract metric counts from different source formats
        if source == "web":
            metric_count = result.get("article_count", 0) + result.get("filing_count", 0)
        elif source == "fundamentals":
            metric_count = result.get("metric_count", 0)
        elif source == "social":
            metric_count = result.get("post_count", 0)
        elif source == "technicals":
            metric_count = result.get("signal_count", 0)

        return {
            "source": source,
            "success": True,
            "markdown": markdown,
            "metric_count": metric_count,
            "error": None,
        }

    except Exception as e:
        return {
            "source": source,
            "success": False,
            "markdown": "",
            "metric_count": 0,
            "error": str(e),
        }


# â”€â”€â”€ Store Integration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def store_source_result(
    ticker: str,
    source: str,
    markdown: str,
    timestamp: str,
    dry_run: bool = False,
) -> dict:
    """Upload a single source's result to DO Spaces.

    Args:
        ticker: Stock ticker symbol
        source: Source name (e.g., 'web', 'fundamentals')
        markdown: The markdown content to upload
        timestamp: ISO timestamp for path construction
        dry_run: If True, skip actual upload

    Returns:
        dict with 'success', 'key', and 'message'
    """
    if dry_run:
        date_str = timestamp[:10]
        key = f"research/{date_str}/{ticker}_{source}.md"
        return {
            "success": True,
            "key": key,
            "message": f"[DRY RUN] Would upload to {key}",
        }

    try:
        from store import upload_to_spaces

        result = upload_to_spaces(
            markdown_content=markdown,
            ticker=ticker,
            source=source,
            timestamp=timestamp,
        )
        return result

    except ImportError:
        return {
            "success": False,
            "key": "",
            "message": "store.py not available â€” cannot upload to Spaces",
        }
    except Exception as e:
        return {
            "success": False,
            "key": "",
            "message": f"Upload failed: {e}",
        }


def trigger_reindex(dry_run: bool = False) -> dict:
    """Trigger KB re-indexing after storing data.

    Args:
        dry_run: If True, skip actual reindex

    Returns:
        dict with 'success' and 'message'
    """
    if dry_run:
        return {"success": True, "message": "[DRY RUN] Would trigger KB reindex"}

    try:
        from store import trigger_kb_reindex
        return trigger_kb_reindex()
    except ImportError:
        return {"success": False, "message": "store.py not available â€” cannot trigger reindex"}
    except Exception as e:
        return {"success": False, "message": f"Reindex failed: {e}"}


# â”€â”€â”€ Orchestrator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def gather(
    ticker: str,
    company_name: str,
    agent: str,
    sources: Optional[list[str]] = None,
    theme: Optional[str] = None,
    directive: Optional[str] = None,
    dry_run: bool = False,
) -> dict:
    """Run the full gather pipeline for one ticker.

    1. Run each source's gather script (with error isolation)
    2. Store each result to Spaces (separate files)
    3. Trigger KB reindex (once at the end)
    4. Return a summary for inter-agent notifications

    Args:
        ticker: Stock ticker symbol
        company_name: Full company name
        agent: Agent name (nova, luna, ace, max)
        sources: List of sources to run (defaults to agent's default sources)
        theme: Optional research theme
        directive: Optional research directive
        dry_run: If True, skip store/reindex

    Returns:
        dict with gather results, store results, and a notification summary
    """
    timestamp = datetime.now(timezone.utc).isoformat()

    # Resolve sources
    if sources is None:
        sources = AGENT_SOURCES.get(agent, [])

    if not sources:
        return {
            "ticker": ticker,
            "company": company_name,
            "agent": agent,
            "timestamp": timestamp,
            "sources": [],
            "gather_results": [],
            "store_results": [],
            "reindex": {"success": False, "message": "No sources to gather"},
            "summary": f"No sources configured for agent '{agent}'",
            "success": False,
        }

    # Step 1: Gather from each source (error-isolated)
    gather_results = []
    for source in sources:
        result = run_source(source, ticker, company_name, theme=theme, directive=directive)
        gather_results.append(result)

        # Small delay between sources to be polite to APIs
        if source != sources[-1]:
            time.sleep(0.2)

    # Step 2: Store successful results to Spaces
    store_results = []
    any_stored = False
    for gr in gather_results:
        if gr["success"] and gr["markdown"]:
            sr = store_source_result(
                ticker=ticker,
                source=gr["source"],
                markdown=gr["markdown"],
                timestamp=timestamp,
                dry_run=dry_run,
            )
            store_results.append(sr)
            if sr["success"]:
                any_stored = True
        else:
            store_results.append({
                "success": False,
                "key": "",
                "message": f"Skipped: {gr['source']} gather failed",
            })

    # Step 3: Trigger KB reindex (once, if anything was stored)
    if any_stored:
        reindex_result = trigger_reindex(dry_run=dry_run)
    else:
        reindex_result = {"success": False, "message": "No data stored â€” skipping reindex"}

    # Step 4: Build notification summary
    succeeded = [gr for gr in gather_results if gr["success"]]
    failed = [gr for gr in gather_results if not gr["success"]]

    summary_parts = []
    for gr in succeeded:
        count = gr["metric_count"]
        label = _source_label(gr["source"], count)
        summary_parts.append(label)

    summary = f"${ticker}: {', '.join(summary_parts)}" if summary_parts else f"${ticker}: no data gathered"

    if failed:
        failed_names = [f["source"] for f in failed]
        summary += f" (failed: {', '.join(failed_names)})"

    return {
        "ticker": ticker,
        "company": company_name,
        "agent": agent,
        "timestamp": timestamp,
        "sources": sources,
        "gather_results": [
            {"source": gr["source"], "success": gr["success"], "metric_count": gr["metric_count"], "error": gr.get("error")}
            for gr in gather_results
        ],
        "store_results": [
            {"source": sources[i] if i < len(sources) else "unknown", "success": sr["success"], "key": sr.get("key", ""), "message": sr.get("message", "")}
            for i, sr in enumerate(store_results)
        ],
        "reindex": reindex_result,
        "summary": summary,
        "success": len(succeeded) > 0,
        "dry_run": dry_run,
    }


def _source_label(source: str, count: int) -> str:
    """Build a human-readable label for a source result."""
    labels = {
        "web": f"{count} articles/filings",
        "fundamentals": f"{count} financial metrics",
        "social": f"{count} social posts",
        "technicals": f"{count} technical signals",
    }
    return labels.get(source, f"{count} items from {source}")


# â”€â”€â”€ CLI Interface â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def main():
    parser = argparse.ArgumentParser(
        description="Gather research data for a ticker (per-agent pipeline)"
    )
    parser.add_argument("--ticker", required=True, help="Stock ticker symbol")
    parser.add_argument("--name", required=True, help="Company name")
    parser.add_argument("--agent", required=True, choices=sorted(VALID_AGENTS), help="Agent name")
    parser.add_argument(
        "--sources",
        help="Comma-separated sources to run (default: agent's defaults). "
             "Options: web, fundamentals, social, technicals",
    )
    parser.add_argument("--theme", help="Research theme")
    parser.add_argument("--directive", help="Research directive")
    parser.add_argument("--dry-run", action="store_true", help="Gather only, skip store/reindex")
    parser.add_argument("--json", action="store_true", help="Output as JSON")
    parser.add_argument("--output", help="Write combined markdown to file")

    args = parser.parse_args()

    ticker = args.ticker.upper().lstrip("$")

    # Parse sources
    sources = None
    if args.sources:
        sources = [s.strip() for s in args.sources.split(",")]
        invalid = [s for s in sources if s not in SOURCE_REGISTRY]
        if invalid:
            print(f"Error: unknown sources: {', '.join(invalid)}", file=sys.stderr)
            print(f"Valid sources: {', '.join(sorted(SOURCE_REGISTRY.keys()))}", file=sys.stderr)
            sys.exit(1)

    # Run
    result = gather(
        ticker=ticker,
        company_name=args.name,
        agent=args.agent,
        sources=sources,
        theme=args.theme,
        directive=args.directive,
        dry_run=args.dry_run,
    )

    # Output
    if args.json:
        # Strip markdown from JSON output (too large)
        print(json.dumps(result, indent=2, default=str))
    else:
        # Print summary
        status = "âœ…" if result["success"] else "âŒ"
        dry = " [DRY RUN]" if result.get("dry_run") else ""
        print(f"{status} {result['agent'].capitalize()} gathered{dry}: {result['summary']}")

        for sr in result.get("store_results", []):
            if sr["success"]:
                print(f"  ğŸ“¦ Stored: {sr.get('key', '')}")
            elif sr.get("message", "").startswith("Skipped"):
                pass  # Don't clutter with skipped messages
            else:
                print(f"  âš ï¸  Store: {sr.get('message', 'unknown error')}")

        reindex = result.get("reindex", {})
        if reindex.get("success"):
            print(f"  ğŸ”„ KB reindex: {reindex.get('message', 'triggered')}")

    # Write combined markdown to file if requested
    if args.output:
        combined = []
        for gr in result.get("gather_results", []):
            # We need to re-run to get markdown since we stripped it from JSON
            pass

        # Actually, let's gather the markdown from the source runs
        # We need to re-read from the full result which we haven't stripped yet
        # This is handled by re-gathering â€” but for --output, let's just
        # run the gather again with the results we have
        print(f"\nğŸ“ Note: use individual gather scripts with --output for full markdown.", file=sys.stderr)

    if not result["success"]:
        sys.exit(1)


if __name__ == "__main__":
    main()
